<div style="text-align: left">
    <mark style="background-color: #ab2333!important"> 
        Transformers: Multi-Head Attention Layers 
    </mark> 
</div>
<hr />

<p><strong>Multi-Head Attention Algorithm</strong>
\(\begin{aligned}
						d_k = d_v = \frac{D}{h} \ | D &amp;= d_{\text{model}}  \\
						W^O &amp;\in \mathbb{R}^{D \times D} \\
						W^Q &amp;\in \mathbb{R}^{D \times d_k} \\
						W^K &amp;\in \mathbb{R}^{D \times d_k} \\
						W^V &amp;\in \mathbb{R}^{D \times d_v} \\ \\
\text{MultiHead(Q,K,V)} &amp;= \text{Concat}(h_i,...,h_h)W^O \\
						h &amp;= \text{Attention}(QW_i^Q,KW_i^K,VW_i^V) \\
\end{aligned}\)</p>
