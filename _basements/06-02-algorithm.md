---
slide: 06-algorithm 
---

<div style="text-align: left">
    <mark style="background-color: #ab2333!important"> 
        Transformers: Attention Layers 
    </mark> 
</div>
---

**Attention Algorithm**
$$\text{Attention(Q,K,V)} = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}}\right) V$$

